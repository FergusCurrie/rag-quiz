{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVerview\n",
    "\n",
    "Just update concepts.txt, then run all. It will had 5 questions and answers to db. \n",
    "\n",
    "# Load concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact Table Dimensional Modelling\n",
      "Fact Table\n",
      "Dimension Table\n",
      "L1 Cache\n",
      "L2 Cache\n",
      "Register\n",
      "XGboost\n",
      "decision tree\n",
      "backpropogation\n",
      "mean squared error\n",
      "mean absolute error\n",
      "root mean squared error\n"
     ]
    }
   ],
   "source": [
    "with open('concepts.txt', 'r') as f:\n",
    "    concepts = [line.strip() for line in f if line.strip()]\n",
    "for c in concepts:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each concept, if doesn't have N qa records, query more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact Table Dimensional Modelling has questions\n",
      "Fact Table has questions\n",
      "Dimension Table has questions\n",
      "L1 Cache has questions\n",
      "L2 Cache has questions\n",
      "Register has questions\n",
      "XGboost has questions\n",
      "decision tree has questions\n",
      "backpropogation has questions\n",
      "mean squared error has questions\n",
      "creating qa for mean absolute error, with concept id = 361362c1-db45-46f3-ad83-9a0db6120f5d\n",
      " using prompt id = 25d1a107-44ae-45dc-a79a-2de72c47eff7\n",
      "    created : q=What is mean absolute error (MAE) and how is it calculated?, a=Mean Absolute Error (MAE) is a measure of the average magnitude of errors in a set of predictions, without considering their direction. It is calculated by taking the average of the absolute differences between predicted and actual values, expressed as MAE = (1/n) * Î£|actual - predicted|, where n is the number of observations.\n",
      "    created : q=Why is MAE considered an important metric in regression analysis?, a=MAE is important in regression analysis because it provides a clear understanding of the average error in predictions, is easy to interpret, and is less sensitive to outliers compared to other metrics like Mean Squared Error (MSE).\n",
      "    created : q=In what situations might MAE be preferred over other error metrics?, a=MAE is preferred when the emphasis is on the average magnitude of errors and when the impact of outliers needs to be minimized, making it particularly useful in applications where uniform error distribution is desired, such as forecasting.\n",
      "    created : q=If a model's predictions resulted in MAE of 5, what does this indicate about the model's performance?, a=An MAE of 5 indicates that, on average, the model's predictions are 5 units away from the actual observed values. This suggests that there is a consistent level of error in the predictions.\n",
      "    created : q=How does the scale of the data affect the interpretation of MAE?, a=The scale of the data significantly affects the interpretation of MAE because it is a unit-specific measure; a MAE of 5 might be acceptable for a dataset with values in the thousands but considered large in a dataset where values range from 0 to 10. Always consider the context and range of values when interpreting MAE.\n",
      "creating qa for root mean squared error, with concept id = d9770d86-0f5f-41fd-ad34-040772a83fe0\n",
      " using prompt id = 25d1a107-44ae-45dc-a79a-2de72c47eff7\n",
      "    created : q=What does the root mean squared error (RMSE) measure in a dataset?, a=RMSE measures the average magnitude of the errors between predicted values and observed values in a dataset, providing an indication of how well a model performs.\n",
      "    created : q=How is RMSE calculated?, a=RMSE is calculated by taking the square root of the average of the squared differences between predicted values and actual values.\n",
      "    created : q=Why is RMSE a preferred metric over Mean Absolute Error (MAE) in some cases?, a=RMSE is preferred when large errors are particularly undesirable, as it squares the errors, thus giving more weight to larger discrepancies compared to MAE.\n",
      "    created : q=What units does RMSE have in relation to the original dataset?, a=RMSE has the same units as the original data since it is derived from the original values through squaring and then taking the square root.\n",
      "    created : q=How can RMSE be used to compare different models?, a=RMSE can be used to compare models by analyzing their RMSE values; a lower RMSE value indicates a better fit to the data and superior predictive performance among the models compared.\n"
     ]
    }
   ],
   "source": [
    "from backend.connections import get_postgres_db\n",
    "\n",
    "from backend.crud import create_concept, create_question\n",
    "from sqlalchemy import text \n",
    "\n",
    "from backend import models \n",
    "from backend.llm_interface.InterfaceCreateQA import InterfaceCreateQA\n",
    "\n",
    "QA_PROMPT_TYPE = 'generate_qa'\n",
    "\n",
    "\n",
    "def check_if_concept_has_qa(concept):\n",
    "    concept_object = next(get_postgres_db()).query(models.ConceptStore).filter(models.ConceptStore.concept == concept).first()\n",
    "    concept_id = concept_object.concept_id\n",
    "    questions_for_concept = next(get_postgres_db()).query(models.Question).filter(models.Question.concept_id == concept_id).all()\n",
    "    if len(questions_for_concept) > 0:\n",
    "        \n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def add_questions_for_concept_if_none_exist(concept):\n",
    "    if check_if_concept_has_qa(concept):\n",
    "        print(f'{concept} has questions')\n",
    "    else:\n",
    "        \n",
    "        # get concept id \n",
    "        concept_object = next(get_postgres_db()).query(models.ConceptStore).filter(models.ConceptStore.concept == concept).first()\n",
    "        concept_id = concept_object.concept_id\n",
    "        print(f'creating qa for {concept}, with concept id = {concept_id}')\n",
    "\n",
    "        # get prompt id \n",
    "        prompt_record = (\n",
    "            next(get_postgres_db()).query(models.PromptStore)\n",
    "            .filter(models.PromptStore.prompt_type == QA_PROMPT_TYPE)\n",
    "            .order_by(models.PromptStore.created_date.desc())\n",
    "            .first()\n",
    "        )\n",
    "        print(f' using prompt id = {prompt_record.prompt_id}')\n",
    "        \n",
    "        # create questions \n",
    "        qa_interface = InterfaceCreateQA(prompt_id=prompt_record.prompt_id, prompt=prompt_record.prompt)\n",
    "        response = qa_interface.create_quiz_for_concept(concept)\n",
    "        for question_number, question_answer in response.items():\n",
    "                create_question(\n",
    "                    next(get_postgres_db()), \n",
    "                    question_answer['question'], \n",
    "                    question_answer['answer'], \n",
    "                    concept_id=concept_id,\n",
    "                    prompt_id=prompt_record.prompt_id\n",
    "                )\n",
    "                print(f'    created : q={question_answer[\"question\"]}, a={question_answer[\"answer\"]}')\n",
    "\n",
    "\n",
    "for c in concepts:\n",
    "     add_questions_for_concept_if_none_exist(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-quiz-_842fax4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
